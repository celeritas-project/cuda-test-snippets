
Fatbin ptx code:
================
arch = sm_70
code version = [7,5]
producer = <unknown>
host = linux
compile_size = 64bit
compressed








.version 7.5
.target sm_70
.address_size 64


.func (.param .b64 func_retval0) __internal_accurate_pow
(
.param .b64 __internal_accurate_pow_param_0
)
;

.visible .entry _Z8cbrt_powPd(
.param .u64 _Z8cbrt_powPd_param_0
)
{
.reg .pred %p<25>;
.reg .b32 %r<27>;
.reg .f64 %fd<21>;
.reg .b64 %rd<5>;


ld.param.u64 %rd2, [_Z8cbrt_powPd_param_0];
cvta.to.global.u64 %rd3, %rd2;
mov.u32 %r5, %tid.x;
mul.wide.u32 %rd4, %r5, 8;
add.s64 %rd1, %rd3, %rd4;
ld.global.f64 %fd1, [%rd1];
{
.reg .b32 %temp; 
mov.b64 {%temp, %r1}, %fd1;
}
mov.f64 %fd12, 0d3FD5555555555555;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r2}, %fd12;
}
and.b32 %r3, %r2, 2146435072;
setp.eq.s32 %p2, %r3, 1127219200;
abs.f64 %fd2, %fd1;
{ 
	.reg .b32 temp_param_reg;
.param .b64 param0;
st.param.f64 [param0+0], %fd2;
.param .b64 retval0;
call.uni (retval0), 
__internal_accurate_pow, 
(
param0
);
ld.param.f64 %fd19, [retval0+0];
} 
	setp.lt.s32 %p3, %r1, 0;
and.pred %p1, %p3, %p2;
not.pred %p4, %p1;
@%p4 bra $L__BB0_2;

{
.reg .b32 %temp; 
mov.b64 {%temp, %r6}, %fd19;
}
xor.b32 %r7, %r6, -2147483648;
{
.reg .b32 %temp; 
mov.b64 {%r8, %temp}, %fd19;
}
mov.b64 %fd19, {%r8, %r7};

$L__BB0_2:
setp.eq.f64 %p5, %fd1, 0d0000000000000000;
@%p5 bra $L__BB0_6;
bra.uni $L__BB0_3;

$L__BB0_6:
selp.b32 %r9, %r1, 0, %p2;
mov.u32 %r10, 0;
or.b32 %r11, %r9, 2146435072;
setp.lt.s32 %p9, %r2, 0;
selp.b32 %r12, %r11, %r9, %p9;
mov.b64 %fd19, {%r10, %r12};
bra.uni $L__BB0_7;

$L__BB0_3:
setp.gt.s32 %p6, %r1, -1;
@%p6 bra $L__BB0_7;

cvt.rzi.f64.f64 %fd14, %fd12;
setp.eq.f64 %p7, %fd14, 0d3FD5555555555555;
@%p7 bra $L__BB0_7;

mov.f64 %fd19, 0dFFF8000000000000;

$L__BB0_7:
add.f64 %fd8, %fd1, 0d3FD5555555555555;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r13}, %fd8;
}
and.b32 %r14, %r13, 2146435072;
setp.ne.s32 %p10, %r14, 2146435072;
mov.f64 %fd20, %fd19;
@%p10 bra $L__BB0_13;

setp.gtu.f64 %p11, %fd2, 0d7FF0000000000000;
mov.f64 %fd20, %fd8;
@%p11 bra $L__BB0_13;

{
.reg .b32 %temp; 
mov.b64 {%r15, %temp}, %fd12;
}
and.b32 %r4, %r2, 2147483647;
setp.eq.s32 %p12, %r4, 2146435072;
setp.eq.s32 %p13, %r15, 0;
and.pred %p14, %p12, %p13;
@%p14 bra $L__BB0_12;
bra.uni $L__BB0_10;

$L__BB0_12:
setp.gt.f64 %p21, %fd2, 0d3FF0000000000000;
selp.b32 %r22, 2146435072, 0, %p21;
mov.u32 %r23, 0;
xor.b32 %r24, %r22, 2146435072;
setp.lt.s32 %p22, %r2, 0;
selp.b32 %r25, %r24, %r22, %p22;
setp.eq.f64 %p23, %fd1, 0dBFF0000000000000;
selp.b32 %r26, 1072693248, %r25, %p23;
mov.b64 %fd20, {%r23, %r26};
bra.uni $L__BB0_13;

$L__BB0_10:
{
.reg .b32 %temp; 
mov.b64 {%r16, %temp}, %fd1;
}
and.b32 %r17, %r1, 2147483647;
setp.ne.s32 %p15, %r17, 2146435072;
setp.ne.s32 %p16, %r16, 0;
or.pred %p17, %p15, %p16;
mov.f64 %fd20, %fd19;
@%p17 bra $L__BB0_13;

setp.gt.s32 %p18, %r2, -1;
selp.b32 %r18, 2146435072, 0, %p18;
mov.u32 %r19, 0;
setp.ne.s32 %p19, %r4, 1071644672;
and.pred %p20, %p19, %p1;
or.b32 %r20, %r18, -2147483648;
selp.b32 %r21, %r20, %r18, %p20;
mov.b64 %fd20, {%r19, %r21};

$L__BB0_13:
setp.eq.f64 %p24, %fd1, 0d3FF0000000000000;
selp.f64 %fd17, 0d3FF0000000000000, %fd20, %p24;
st.global.f64 [%rd1], %fd17;
ret;

}
.func (.param .b64 func_retval0) __internal_accurate_pow(
.param .b64 __internal_accurate_pow_param_0
)
{
.reg .pred %p<10>;
.reg .f32 %f<3>;
.reg .b32 %r<53>;
.reg .f64 %fd<138>;


ld.param.f64 %fd12, [__internal_accurate_pow_param_0];
{
.reg .b32 %temp; 
mov.b64 {%temp, %r50}, %fd12;
}
{
.reg .b32 %temp; 
mov.b64 {%r49, %temp}, %fd12;
}
shr.u32 %r51, %r50, 20;
setp.ne.s32 %p1, %r51, 0;
@%p1 bra $L__BB1_2;

mul.f64 %fd13, %fd12, 0d4350000000000000;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r50}, %fd13;
}
{
.reg .b32 %temp; 
mov.b64 {%r49, %temp}, %fd13;
}
shr.u32 %r16, %r50, 20;
add.s32 %r51, %r16, -54;

$L__BB1_2:
add.s32 %r52, %r51, -1023;
and.b32 %r17, %r50, -2146435073;
or.b32 %r18, %r17, 1072693248;
mov.b64 %fd135, {%r49, %r18};
setp.lt.u32 %p2, %r18, 1073127583;
@%p2 bra $L__BB1_4;

{
.reg .b32 %temp; 
mov.b64 {%r19, %temp}, %fd135;
}
{
.reg .b32 %temp; 
mov.b64 {%temp, %r20}, %fd135;
}
add.s32 %r21, %r20, -1048576;
mov.b64 %fd135, {%r19, %r21};
add.s32 %r52, %r51, -1022;

$L__BB1_4:
add.f64 %fd14, %fd135, 0d3FF0000000000000;
mov.f64 %fd15, 0d3FF0000000000000;
rcp.approx.ftz.f64 %fd16, %fd14;
neg.f64 %fd17, %fd14;
fma.rn.f64 %fd18, %fd17, %fd16, %fd15;
fma.rn.f64 %fd19, %fd18, %fd18, %fd18;
fma.rn.f64 %fd20, %fd19, %fd16, %fd16;
add.f64 %fd21, %fd135, 0dBFF0000000000000;
mul.f64 %fd22, %fd21, %fd20;
fma.rn.f64 %fd23, %fd21, %fd20, %fd22;
mul.f64 %fd24, %fd23, %fd23;
mov.f64 %fd25, 0d3ED0F5D241AD3B5A;
mov.f64 %fd26, 0d3EB0F5FF7D2CAFE2;
fma.rn.f64 %fd27, %fd26, %fd24, %fd25;
mov.f64 %fd28, 0d3EF3B20A75488A3F;
fma.rn.f64 %fd29, %fd27, %fd24, %fd28;
mov.f64 %fd30, 0d3F1745CDE4FAECD5;
fma.rn.f64 %fd31, %fd29, %fd24, %fd30;
mov.f64 %fd32, 0d3F3C71C7258A578B;
fma.rn.f64 %fd33, %fd31, %fd24, %fd32;
mov.f64 %fd34, 0d3F6249249242B910;
fma.rn.f64 %fd35, %fd33, %fd24, %fd34;
mov.f64 %fd36, 0d3F89999999999DFB;
fma.rn.f64 %fd37, %fd35, %fd24, %fd36;
sub.f64 %fd38, %fd21, %fd23;
add.f64 %fd39, %fd38, %fd38;
neg.f64 %fd40, %fd23;
fma.rn.f64 %fd41, %fd40, %fd21, %fd39;
mul.f64 %fd42, %fd20, %fd41;
fma.rn.f64 %fd43, %fd24, %fd37, 0d3FB5555555555555;
mov.f64 %fd44, 0d3FB5555555555555;
sub.f64 %fd45, %fd44, %fd43;
fma.rn.f64 %fd46, %fd24, %fd37, %fd45;
add.f64 %fd47, %fd46, 0d0000000000000000;
add.f64 %fd48, %fd47, 0dBC46A4CB00B9E7B0;
add.f64 %fd49, %fd43, %fd48;
sub.f64 %fd50, %fd43, %fd49;
add.f64 %fd51, %fd48, %fd50;
mul.rn.f64 %fd52, %fd23, %fd23;
neg.f64 %fd53, %fd52;
fma.rn.f64 %fd54, %fd23, %fd23, %fd53;
{
.reg .b32 %temp; 
mov.b64 {%r22, %temp}, %fd42;
}
{
.reg .b32 %temp; 
mov.b64 {%temp, %r23}, %fd42;
}
add.s32 %r24, %r23, 1048576;
mov.b64 %fd55, {%r22, %r24};
fma.rn.f64 %fd56, %fd23, %fd55, %fd54;
mul.rn.f64 %fd57, %fd52, %fd23;
neg.f64 %fd58, %fd57;
fma.rn.f64 %fd59, %fd52, %fd23, %fd58;
fma.rn.f64 %fd60, %fd52, %fd42, %fd59;
fma.rn.f64 %fd61, %fd56, %fd23, %fd60;
mul.rn.f64 %fd62, %fd49, %fd57;
neg.f64 %fd63, %fd62;
fma.rn.f64 %fd64, %fd49, %fd57, %fd63;
fma.rn.f64 %fd65, %fd49, %fd61, %fd64;
fma.rn.f64 %fd66, %fd51, %fd57, %fd65;
add.f64 %fd67, %fd62, %fd66;
sub.f64 %fd68, %fd62, %fd67;
add.f64 %fd69, %fd66, %fd68;
add.f64 %fd70, %fd23, %fd67;
sub.f64 %fd71, %fd23, %fd70;
add.f64 %fd72, %fd67, %fd71;
add.f64 %fd73, %fd69, %fd72;
add.f64 %fd74, %fd42, %fd73;
add.f64 %fd75, %fd70, %fd74;
sub.f64 %fd76, %fd70, %fd75;
add.f64 %fd77, %fd74, %fd76;
xor.b32 %r25, %r52, -2147483648;
mov.u32 %r26, -2147483648;
mov.u32 %r27, 1127219200;
mov.b64 %fd78, {%r25, %r27};
mov.b64 %fd79, {%r26, %r27};
sub.f64 %fd80, %fd78, %fd79;
mov.f64 %fd81, 0d3FE62E42FEFA39EF;
fma.rn.f64 %fd82, %fd80, %fd81, %fd75;
neg.f64 %fd83, %fd80;
fma.rn.f64 %fd84, %fd83, %fd81, %fd82;
sub.f64 %fd85, %fd84, %fd75;
sub.f64 %fd86, %fd77, %fd85;
mov.f64 %fd87, 0d3C7ABC9E3B39803F;
fma.rn.f64 %fd88, %fd80, %fd87, %fd86;
add.f64 %fd89, %fd82, %fd88;
sub.f64 %fd90, %fd82, %fd89;
add.f64 %fd91, %fd88, %fd90;
mov.f64 %fd92, 0d3FD5555555555555;
{
.reg .b32 %temp; 
mov.b64 {%temp, %r28}, %fd92;
}
shl.b32 %r29, %r28, 1;
setp.gt.u32 %p3, %r29, -33554433;
and.b32 %r30, %r28, -15728641;
selp.b32 %r31, %r30, %r28, %p3;
{
.reg .b32 %temp; 
mov.b64 {%r32, %temp}, %fd92;
}
mov.b64 %fd93, {%r32, %r31};
mul.rn.f64 %fd94, %fd89, %fd93;
neg.f64 %fd95, %fd94;
fma.rn.f64 %fd96, %fd89, %fd93, %fd95;
fma.rn.f64 %fd97, %fd91, %fd93, %fd96;
add.f64 %fd4, %fd94, %fd97;
sub.f64 %fd98, %fd94, %fd4;
add.f64 %fd5, %fd97, %fd98;
mov.f64 %fd99, 0d4338000000000000;
mov.f64 %fd100, 0d3FF71547652B82FE;
fma.rn.f64 %fd101, %fd4, %fd100, %fd99;
{
.reg .b32 %temp; 
mov.b64 {%r13, %temp}, %fd101;
}
mov.f64 %fd102, 0dC338000000000000;
add.rn.f64 %fd103, %fd101, %fd102;
mov.f64 %fd104, 0dBFE62E42FEFA39EF;
fma.rn.f64 %fd105, %fd103, %fd104, %fd4;
mov.f64 %fd106, 0dBC7ABC9E3B39803F;
fma.rn.f64 %fd107, %fd103, %fd106, %fd105;
mov.f64 %fd108, 0d3E928AF3FCA213EA;
mov.f64 %fd109, 0d3E5ADE1569CE2BDF;
fma.rn.f64 %fd110, %fd109, %fd107, %fd108;
mov.f64 %fd111, 0d3EC71DEE62401315;
fma.rn.f64 %fd112, %fd110, %fd107, %fd111;
mov.f64 %fd113, 0d3EFA01997C89EB71;
fma.rn.f64 %fd114, %fd112, %fd107, %fd113;
mov.f64 %fd115, 0d3F2A01A014761F65;
fma.rn.f64 %fd116, %fd114, %fd107, %fd115;
mov.f64 %fd117, 0d3F56C16C1852B7AF;
fma.rn.f64 %fd118, %fd116, %fd107, %fd117;
mov.f64 %fd119, 0d3F81111111122322;
fma.rn.f64 %fd120, %fd118, %fd107, %fd119;
mov.f64 %fd121, 0d3FA55555555502A1;
fma.rn.f64 %fd122, %fd120, %fd107, %fd121;
mov.f64 %fd123, 0d3FC5555555555511;
fma.rn.f64 %fd124, %fd122, %fd107, %fd123;
mov.f64 %fd125, 0d3FE000000000000B;
fma.rn.f64 %fd126, %fd124, %fd107, %fd125;
fma.rn.f64 %fd127, %fd126, %fd107, %fd15;
fma.rn.f64 %fd128, %fd127, %fd107, %fd15;
{
.reg .b32 %temp; 
mov.b64 {%r14, %temp}, %fd128;
}
{
.reg .b32 %temp; 
mov.b64 {%temp, %r15}, %fd128;
}
shl.b32 %r33, %r13, 20;
add.s32 %r34, %r15, %r33;
mov.b64 %fd136, {%r14, %r34};
{
.reg .b32 %temp; 
mov.b64 {%temp, %r35}, %fd4;
}
mov.b32 %f2, %r35;
abs.ftz.f32 %f1, %f2;
setp.lt.ftz.f32 %p4, %f1, 0f4086232B;
@%p4 bra $L__BB1_7;

setp.lt.f64 %p5, %fd4, 0d0000000000000000;
add.f64 %fd129, %fd4, 0d7FF0000000000000;
selp.f64 %fd136, 0d0000000000000000, %fd129, %p5;
setp.geu.ftz.f32 %p6, %f1, 0f40874800;
@%p6 bra $L__BB1_7;

mov.f64 %fd134, 0d4338000000000000;
mov.f64 %fd133, 0d3FF71547652B82FE;
fma.rn.f64 %fd132, %fd4, %fd133, %fd134;
{
.reg .b32 %temp; 
mov.b64 {%r48, %temp}, %fd132;
}
shr.u32 %r36, %r48, 31;
add.s32 %r37, %r48, %r36;
shr.s32 %r38, %r37, 1;
shl.b32 %r39, %r38, 20;
add.s32 %r40, %r15, %r39;
mov.b64 %fd130, {%r14, %r40};
sub.s32 %r41, %r48, %r38;
shl.b32 %r42, %r41, 20;
add.s32 %r43, %r42, 1072693248;
mov.u32 %r44, 0;
mov.b64 %fd131, {%r44, %r43};
mul.f64 %fd136, %fd130, %fd131;

$L__BB1_7:
{
.reg .b32 %temp; 
mov.b64 {%temp, %r45}, %fd136;
}
and.b32 %r46, %r45, 2147483647;
setp.eq.s32 %p7, %r46, 2146435072;
{
.reg .b32 %temp; 
mov.b64 {%r47, %temp}, %fd136;
}
setp.eq.s32 %p8, %r47, 0;
and.pred %p9, %p8, %p7;
@%p9 bra $L__BB1_9;

fma.rn.f64 %fd136, %fd136, %fd5, %fd136;

$L__BB1_9:
st.param.f64 [func_retval0+0], %fd136;
ret;

}

